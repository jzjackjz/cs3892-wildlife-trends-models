{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, sampler\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import inf\n",
    "from merge_datasets import MergeDatasets\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torch import optim, cuda, Tensor\n",
    "\n",
    "# Data science tools\n",
    "import numpy as np\n",
    "\n",
    "# Image manipulations\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams['font.size'] = 14\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class FFNNmodel(nn.Module):\n",
    "    def __init__(self, input_d, hidden_d, output_d):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_d, hidden_d)\n",
    "        self.linear2 = nn.Linear(hidden_d, output_d)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_df = pd.read_csv('../datasets/co2-long-term-concentration.csv')\n",
    "forest_df = pd.read_csv('../datasets/forest-area-km.csv')\n",
    "land_df = pd.read_csv('../datasets/land-use-over-the-long-term.csv')\n",
    "temp_df = pd.read_csv('../datasets/annual-temperature-anomalies.csv')\n",
    "invas_df = pd.read_csv('../datasets/budget-to-manage-invasive-alien-species.csv')\n",
    "lpi_df = pd.read_csv('../datasets/global-living-planet-index.csv') # target\n",
    "\n",
    "merge_datasets = MergeDatasets(co2_df, forest_df, land_df, temp_df, invas_df, lpi_df)\n",
    "merged_df = merge_datasets.merge()\n",
    "\n",
    "features = ['Year', 'Entity', 'Long-run CO₂ concentration', 'Forest area', 'Land use: Built-up area', \n",
    "            'Land use: Grazingland', 'Land use: Cropland', 'Temperature anomaly']\n",
    "\n",
    "entities = pd.get_dummies(merged_df['Entity'])\n",
    "X_continuous = merged_df[['Year', 'Long-run CO₂ concentration', 'Forest area', 'Land use: Built-up area', \n",
    "                          'Land use: Grazingland', 'Land use: Cropland', 'Temperature anomaly']].values\n",
    "X = np.hstack((X_continuous, entities.values))\n",
    "\n",
    "y = merged_df['Living Planet Index'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Scale labels\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()  # Flatten to 1D array after scaling\n",
    "\n",
    "# Convert scaled features and labels to tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_scaled, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "input_d = X.shape[1]\n",
    "hidden_d = 64\n",
    "output_d = 1\n",
    "\n",
    "model = FFNNmodel(input_d, hidden_d, output_d)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          save_file_name,\n",
    "          max_epochs_stop=10,\n",
    "          n_epochs=10,\n",
    "          print_every=1):\n",
    "    \"\"\"Train a PyTorch Model\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        model (PyTorch model): cnn to train\n",
    "        criterion (PyTorch loss): objective to minimize\n",
    "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
    "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
    "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
    "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
    "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
    "        n_epochs (int): maximum number of training epochs\n",
    "        print_every (int): frequency of epochs to print training stats\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        model (PyTorch model): trained cnn with best weights\n",
    "        history (DataFrame): history of train and validation loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Early stopping intialization\n",
    "    epochs_no_improve = 0\n",
    "    valid_loss_min = np.Inf\n",
    "    history = []\n",
    "\n",
    "    # Number of epochs already trained (if using loaded in model weights)\n",
    "    try:\n",
    "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
    "    except:\n",
    "        model.epochs = 0\n",
    "        print(f'Starting Training from Scratch.\\n')\n",
    "\n",
    "    overall_start = timer()\n",
    "\n",
    "    # Main loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # keep track of training and validation loss each epoch\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        # Set to training\n",
    "        model.train()\n",
    "        start = timer()\n",
    "\n",
    "        # Training loop\n",
    "        for ii, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            # Tensors to gpu, both model parameters, data, and target need to be tensors.\n",
    "            # data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward path\n",
    "            output = model(data)\n",
    "\n",
    "            # Loss function\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward path (backpropagation)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track train loss by multiplying average loss by number of examples in batch\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Calculate accuracy by finding max log probability\n",
    "            # _, pred = torch.max(output, dim=1)\n",
    "            # correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "\n",
    "            # # Need to convert correct tensor from int to float to average\n",
    "            # accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "\n",
    "            # # Multiply average accuracy times the number of examples in batch\n",
    "            # train_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "            # # Track training progress\n",
    "            # print(\n",
    "            #     f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
    "            #     end='\\r')\n",
    "\n",
    "        # After training loops ends, start validation\n",
    "        else:\n",
    "            model.epochs += 1\n",
    "\n",
    "            # Don't need to keep track of gradients\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Set to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                # Validation loop\n",
    "                for data, target in valid_loader:\n",
    "                    # Tensors to gpu\n",
    "                    # data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                    # Forward path\n",
    "                    output = model(data)\n",
    "\n",
    "                    # Validation loss computation\n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    # Multiply average loss times the number of examples in batch\n",
    "                    valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                    # Calculate validation accuracy\n",
    "                    _, pred = torch.max(output, dim=1)\n",
    "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                    accuracy = torch.mean(\n",
    "                    correct_tensor.type(torch.FloatTensor))\n",
    "\n",
    "                # Calculate average losses and Calculate average accuracy\n",
    "                train_loss = train_loss / len(train_loader.dataset)\n",
    "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "                history.append([train_loss, valid_loss])\n",
    "\n",
    "                # Print training and validation results\n",
    "                if (epoch + 1) % print_every == 0:\n",
    "                    print(\n",
    "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
    "                    )\n",
    "\n",
    "                # Save the model if validation loss decreases\n",
    "                if valid_loss < valid_loss_min:\n",
    "                    # Save model\n",
    "                    torch.save(model.state_dict(), save_file_name)\n",
    "\n",
    "                    # Track improvement\n",
    "                    epochs_no_improve = 0\n",
    "                    valid_loss_min = valid_loss\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                # Otherwise increment count of epochs with no improvement\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= max_epochs_stop:\n",
    "                        print(f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f}')\n",
    "                        total_time = timer() - overall_start\n",
    "                        print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.')\n",
    "\n",
    "                        # Load the best state dict\n",
    "                        model.load_state_dict(torch.load(save_file_name))\n",
    "                        model.optimizer = optimizer\n",
    "\n",
    "                        # Format history\n",
    "                        history = pd.DataFrame(history, columns=['train_loss', 'valid_loss'])\n",
    "                        return model, history\n",
    "\n",
    "    # Attach the optimizer\n",
    "    model.optimizer = optimizer\n",
    "    \n",
    "    # Record overall time and print out stats\n",
    "    total_time = timer() - overall_start\n",
    "    print(f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f}')\n",
    "    print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.')\n",
    "\n",
    "    # Format history\n",
    "    history = pd.DataFrame(\n",
    "        history,\n",
    "        columns=['train_loss', 'valid_loss'])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training from Scratch.\n",
      "\n",
      "\n",
      "Epoch: 0 \tTraining Loss: 1.1509 \tValidation Loss: 0.6283\n",
      "\n",
      "Epoch: 1 \tTraining Loss: 1.0036 \tValidation Loss: 0.5656\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 0.9913 \tValidation Loss: 0.5076\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 0.8234 \tValidation Loss: 0.4592\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 0.7213 \tValidation Loss: 0.4097\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 0.7193 \tValidation Loss: 0.3657\n",
      "\n",
      "Epoch: 6 \tTraining Loss: 0.5929 \tValidation Loss: 0.3270\n",
      "\n",
      "Epoch: 7 \tTraining Loss: 0.5943 \tValidation Loss: 0.2898\n",
      "\n",
      "Epoch: 8 \tTraining Loss: 0.5236 \tValidation Loss: 0.2593\n",
      "\n",
      "Epoch: 9 \tTraining Loss: 0.4043 \tValidation Loss: 0.2294\n",
      "\n",
      "Best epoch: 9 with loss: 0.23\n",
      "0.05 total seconds elapsed. 0.00 seconds per epoch.\n",
      "Final Training MSE: 0.4042980411778326\n",
      "Final Validation MSE: 0.22944281228951047\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # You can adjust the batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model\n",
    "input_d = X.shape[1]  # Adjust the input dimension to match feature number\n",
    "hidden_d = 64  # Hidden dimension, can be tuned\n",
    "output_d = 1   # Since it's regression, the output dimension is 1\n",
    "\n",
    "model = bmodel(input_d, hidden_d, output_d)\n",
    "criterion = nn.MSELoss()  # MSE is typically used for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the file name to save the model\n",
    "save_file_name = 'best_model.pt'\n",
    "\n",
    "# Train the model\n",
    "trained_model, history = train(model=model,\n",
    "                               criterion=criterion,\n",
    "                               optimizer=optimizer,\n",
    "                               train_loader=train_loader,\n",
    "                               valid_loader=valid_loader,\n",
    "                               save_file_name=save_file_name,\n",
    "                               max_epochs_stop=3,\n",
    "                               n_epochs=10,\n",
    "                               print_every=1)\n",
    "\n",
    "# Compute the MSE\n",
    "# This will be computed as part of the training function in your current setup, printed out, and saved in 'history'\n",
    "print(f\"Final Training MSE: {history['train_loss'].iloc[-1]}\")\n",
    "print(f\"Final Validation MSE: {history['valid_loss'].iloc[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4262",
   "language": "python",
   "name": "cs4262"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
